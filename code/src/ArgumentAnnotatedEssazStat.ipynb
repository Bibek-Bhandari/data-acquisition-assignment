{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import nltk.data\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\meank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJSonObject():\n",
    "    essayStringList = ''\n",
    "    with open('../data/data.json', errors='ignore') as f:\n",
    "        for jsonObj in f:\n",
    "            essayStringList = essayStringList + jsonObj\n",
    "    jsonEssayObject = json.loads(essayStringList)\n",
    "    return jsonEssayObject\n",
    "\n",
    "\n",
    "def getAllConfirmationBiasEssay():\n",
    "    allConfirmationBiasEssayList = [d for d in jsonEssaysObject if 'confirmation_bias' in d and d['confirmation_bias']\n",
    "                                    is True]\n",
    "    return allConfirmationBiasEssayList\n",
    "\n",
    "\n",
    "def getAllNonConfirmationBiasEssay():\n",
    "    allNonConfirmationBiasEssayList = [d for d in jsonEssaysObject if\n",
    "                                       'confirmation_bias' in d and d['confirmation_bias']\n",
    "                                       is False]\n",
    "    return allNonConfirmationBiasEssayList\n",
    "\n",
    "\n",
    "def getAllSufficientParagraphs():\n",
    "    paragraphsListEachEssay = [d['paragraphs'] for d in jsonEssaysObject if 'paragraphs' in d]\n",
    "    sufficientParagraphsList = [t['text'] for d in paragraphsListEachEssay for t in d if\n",
    "                                'text' in t and t['sufficient'] is True]\n",
    "    return sufficientParagraphsList\n",
    "\n",
    "\n",
    "def getAllNonSufficientParagraphs():\n",
    "    paragraphsListEachEssay = [d['paragraphs'] for d in jsonEssaysObject if 'paragraphs' in d]\n",
    "    nonSufficientParagraphsList = [t['text'] for d in paragraphsListEachEssay for t in d if\n",
    "                                   'text' in t and t['sufficient'] is False]\n",
    "    return nonSufficientParagraphsList\n",
    "\n",
    "\n",
    "def getAllParagraphsTextList():\n",
    "    paragraphsListEachEssay = [d['paragraphs'] for d in jsonEssaysObject if 'paragraphs' in d]\n",
    "    paragraphsList = [t['text'] for d in paragraphsListEachEssay for t in d if 'text' in t]\n",
    "    return paragraphsList\n",
    "\n",
    "\n",
    "def gatAllEssaySentences():\n",
    "    eachEssayTextList = [d['text'] for d in jsonEssaysObject if 'text' in d]\n",
    "    sentenceList = [sentence for text in eachEssayTextList for sentence in tokenizer.tokenize(text)]\n",
    "    return sentenceList\n",
    "\n",
    "\n",
    "def getAllMajorClaims():\n",
    "    majorClaimsInListEachEssay = [d['major_claim'] for d in jsonEssaysObject if 'major_claim' in d]\n",
    "    allMajorClaimsList = [eachMajorClaims for eachEssayMajorClaims in majorClaimsInListEachEssay for eachMajorClaims in\n",
    "                          eachEssayMajorClaims]\n",
    "    return allMajorClaimsList\n",
    "\n",
    "\n",
    "def getAllMajorClaimsTokens():\n",
    "    allMajorClaimsList = getAllMajorClaims()\n",
    "    allMajorClaimsTextList = [eachMajorClaims['text'] for eachMajorClaims in allMajorClaimsList]\n",
    "    allMajorClaimsTokens = [token.text for eachMajorClaimsText in allMajorClaimsTextList for token in\n",
    "                            nlp(eachMajorClaimsText)]\n",
    "    return allMajorClaimsTokens\n",
    "\n",
    "\n",
    "def getAllClaims():\n",
    "    claimsInListEachEssay = [d['claims'] for d in jsonEssaysObject if 'claims' in d]\n",
    "    allClaimsList = [eachClaims for eachEssayClaims in claimsInListEachEssay for eachClaims in\n",
    "                     eachEssayClaims]\n",
    "    return allClaimsList\n",
    "\n",
    "\n",
    "def getAllClaimsTokens():\n",
    "    allClaimsList = getAllClaims()\n",
    "    allClaimsTextList = [eachClaims['text'] for eachClaims in allClaimsList]\n",
    "    allClaimsTokens = [token.text for eachClaimsText in allClaimsTextList for token in\n",
    "                       nlp(eachClaimsText)]\n",
    "    return allClaimsTokens\n",
    "\n",
    "\n",
    "def getAllPremises():\n",
    "    premisesInListEachEssay = [d['premises'] for d in jsonEssaysObject if 'premises' in d]\n",
    "    allPremisesList = [eachPremises for eachEssayPremises in premisesInListEachEssay for eachPremises in\n",
    "                       eachEssayPremises]\n",
    "    return allPremisesList\n",
    "\n",
    "\n",
    "def getAllPremisesTokens():\n",
    "    allPremisesList = getAllPremises()\n",
    "    allPremisesTextList = [eachPremises['text'] for eachPremises in allPremisesList]\n",
    "    allPremisesTokens = [token.text for eachPremisesText in allPremisesTextList for token in\n",
    "                         nlp(eachPremisesText)]\n",
    "    return allPremisesTokens\n",
    "\n",
    "\n",
    "# createEssayObject\n",
    "jsonEssaysObject = getJSonObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of essays = 322\n",
      "Number of paragraphs = 820\n",
      "Number of Sentences = 4432\n",
      "Number of Tokens = 112102\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1.Number of essays, paragraphs, sentences, and tokens\n",
    "def printNoOfEPST():\n",
    "    print('Number of essays = ' + str(len(jsonEssaysObject)))\n",
    "    paragraphsList = getAllParagraphsTextList()\n",
    "    print('Number of paragraphs = ' + str(len(paragraphsList)))\n",
    "    sentenceList = gatAllEssaySentences()\n",
    "    print('Number of Sentences = ' + str(len(sentenceList)))\n",
    "    tokenList = [token.text for sentence in sentenceList for token in nlp(sentence)]\n",
    "    print('Number of Tokens = ' + str(len(tokenList)))\n",
    "\n",
    "\n",
    "printNoOfEPST()\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of major claims =  597\n",
      "Number of claims =  1799\n",
      "Number of premises =  3023\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2.Number of major claims, claims, premises\n",
    "def printNoOfMajorClaimsClaimsPremises():\n",
    "    allMajorClaimsList = getAllMajorClaims()\n",
    "    print('Number of major claims = ', len(allMajorClaimsList))\n",
    "\n",
    "    allClaimsList = getAllClaims()\n",
    "    print('Number of claims = ', len(allClaimsList))\n",
    "\n",
    "    allPremisesList = getAllPremises()\n",
    "    print('Number of premises = ', len(allPremisesList))\n",
    "\n",
    "\n",
    "printNoOfMajorClaimsClaimsPremises()\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of essays with confirmation bias =  122\n",
      "Number of essays without confirmation bias =  200\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.Number of essays with and without confirmation bias\n",
    "def printNoOfWithWithoutConfirmationBiasEssay():\n",
    "    print('Number of essays with confirmation bias = ', len(getAllConfirmationBiasEssay()))\n",
    "    print('Number of essays without confirmation bias = ', len(getAllNonConfirmationBiasEssay()))\n",
    "\n",
    "\n",
    "printNoOfWithWithoutConfirmationBiasEssay()\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sufficient paragraphs =  538\n",
      "Number of insufficient paragraphs =  282\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 4.Number of sufficient and insufficient paragraphs (arguments)\n",
    "def printNoOfSufficientAndInsufficientParagraph():\n",
    "    print('Number of sufficient paragraphs = ', len(getAllSufficientParagraphs()))\n",
    "    print('Number of insufficient paragraphs = ', len(getAllNonSufficientParagraphs()))\n",
    "\n",
    "\n",
    "printNoOfSufficientAndInsufficientParagraph()\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens in major claims, claims, and premises =  23586\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 5.Average number of tokens in major claims, claims, and premises\n",
    "def printAverageNoOfMajorClaimsClaimsPremisesTokens():\n",
    "    allMajorClaimsTokens = getAllMajorClaimsTokens()\n",
    "    allClaimsTokens = getAllMajorClaimsTokens()\n",
    "    allPremisesTokens = getAllPremisesTokens()\n",
    "    totalMCPTokens = len(allMajorClaimsTokens) + len(allClaimsTokens) + len(allPremisesTokens)\n",
    "    print(\"Average number of tokens in major claims, claims, and premises = \", int(totalMCPTokens / 3))\n",
    "\n",
    "\n",
    "printAverageNoOfMajorClaimsClaimsPremisesTokens()\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most specific words in major claims: ['means', 'attention', 'computers', 'choice', 'classroom', 'adult', 'financial', 'energy', 'helping', 'childhood']\n",
      "10 most specific words in claims: ['disagree', 'crime', 'cheap', 'agree', 'budgets', 'eat', 'developed', 'rates', 'exercise', 'indispensable']\n",
      "10 most specific words in premises: ['people', 'good', 'money', 'like', 'help', 'knowledge', 'friends', 'education', 'learn', 'new']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The 10 most specific words in major claims, claims, and premises.\n",
    "def printSpecificWordsInMajorClaimsClaimsPremises():\n",
    "    allMajorClaimsTokens = removePunctAndStopWords(getAllMajorClaimsTokens())\n",
    "    allClaimsTokens = removePunctAndStopWords(getAllClaimsTokens())\n",
    "    allPremisesTokens = removePunctAndStopWords(getAllPremisesTokens())\n",
    "    mt = allMajorClaimsTokens\n",
    "    ct = allClaimsTokens\n",
    "    pt = allPremisesTokens\n",
    "\n",
    "    # Calculating most specific words for Major Claims\n",
    "\n",
    "    for item in mt:\n",
    "        if item in ct or item in pt:\n",
    "            for it in allMajorClaimsTokens:\n",
    "                if item == it:\n",
    "                    allMajorClaimsTokens.remove(it)\n",
    "\n",
    "\n",
    "    specificWordsMajorClaims = []\n",
    "    for word1 in Counter(allMajorClaimsTokens).most_common(10):\n",
    "        specificWordsMajorClaims.append(word1[0])\n",
    "\n",
    "    # Calculating most specific words for  Claims\n",
    "    for item in ct:\n",
    "        if item in mt or item in pt:\n",
    "            for it in allClaimsTokens:\n",
    "                if item == it:\n",
    "                    allClaimsTokens.remove(it)\n",
    "\n",
    "    \n",
    "    specificWordsClaims = []\n",
    "    for word1 in Counter(allClaimsTokens).most_common(10):\n",
    "        specificWordsClaims.append(word1[0])\n",
    "\n",
    "    # Calculating most specific words for Major Claims\n",
    "    for item in pt:\n",
    "        if item in ct or item in mt:\n",
    "            for it in allPremisesTokens:\n",
    "                if item == it:\n",
    "                    allPremisesTokens.remove(it)\n",
    "\n",
    "\n",
    "    \n",
    "    specificWordsPremises = []\n",
    "    for word1 in Counter(allPremisesTokens).most_common(10):\n",
    "        specificWordsPremises.append(word1[0])\n",
    "\n",
    "\n",
    "    print(\"10 most specific words in major claims:\",specificWordsMajorClaims)\n",
    "    print(\"10 most specific words in claims:\",specificWordsClaims)\n",
    "    print(\"10 most specific words in premises:\",specificWordsPremises)\n",
    "\n",
    "\n",
    "def removePunctAndStopWords(list):\n",
    "    out = []\n",
    "    for word in list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False and lexeme.is_punct == False:\n",
    "            out.append(word.lower())\n",
    "    return out\n",
    "printSpecificWordsInMajorClaimsClaimsPremises()\n",
    "print('-' * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
